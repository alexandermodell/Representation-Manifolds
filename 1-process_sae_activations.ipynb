{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e38abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set custom HF_HOME location\n",
    "\n",
    "# Set a custom cache directory (relative to current working directory)\n",
    "os.environ[\"HF_HOME\"] = \"./.hf_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"./.hf_cache/hub\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login to Huggingface and make sure you have be granted access to the Mistral-7B-v0.1 model.\n",
    "login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca46ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "# from huggingface_hub import hf_hub_download, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_sae(layer):\n",
    "    return SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",  # won't always be a hook point\n",
    "        device=DEVICE\n",
    "    )[0]\n",
    "\n",
    "def get_mistral_sae(layer):\n",
    "    from sae_lens import SAE\n",
    "\n",
    "    return SAE.from_pretrained(\n",
    "        release=\"mistral-7b-res-wg\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",  # won't always be a hook point\n",
    "        device=DEVICE\n",
    "    )[0]\n",
    "\n",
    "def get_cluster_activations_gpt2(sparse_sae_activations, sae_neurons_in_cluster, decoder_vecs):\n",
    "    current_token = None\n",
    "    all_activations = []\n",
    "    all_token_indices = []\n",
    "    updated = False\n",
    "    for sae_value, sae_index, token_index in tqdm(zip(\n",
    "        sparse_sae_activations[\"sparse_sae_values\"],\n",
    "        sparse_sae_activations[\"sparse_sae_indices\"],\n",
    "        sparse_sae_activations[\"all_token_indices\"],\n",
    "    ), total = len(sparse_sae_activations[\"sparse_sae_values\"]), disable=True):\n",
    "        if current_token == None:\n",
    "            current_token = token_index\n",
    "            current_activations = np.zeros(768)\n",
    "        if token_index != current_token:\n",
    "            if updated:\n",
    "                all_activations.append(current_activations)\n",
    "                all_token_indices.append(token_index)\n",
    "            updated = False\n",
    "            current_token = token_index\n",
    "            current_activations = np.zeros(768)\n",
    "        if sae_index in sae_neurons_in_cluster:\n",
    "            updated = True\n",
    "            current_activations += sae_value * decoder_vecs[sae_index]\n",
    "\n",
    "    return np.stack(all_activations), all_token_indices\n",
    "\n",
    "\n",
    "def get_cluster_activations_mistral(\n",
    "    sparse_sae_activations,\n",
    "    sae_neurons_in_cluster,\n",
    "    decoder_vecs,\n",
    "    sample_limit,\n",
    "    max_indices=1e8,\n",
    "):\n",
    "    max_indices = int(max_indices)\n",
    "    current_token = None\n",
    "    all_activations = []\n",
    "    all_token_indices = []\n",
    "    updated = False\n",
    "    for sae_value, sae_index, token_index in tqdm(\n",
    "        islice(\n",
    "            zip(\n",
    "                sparse_sae_activations[\"sparse_sae_values\"],\n",
    "                sparse_sae_activations[\"sparse_sae_indices\"],\n",
    "                sparse_sae_activations[\"all_token_indices\"],\n",
    "            ),\n",
    "            0,\n",
    "            max_indices,\n",
    "        ),\n",
    "        total=max_indices,\n",
    "        disable=False,\n",
    "    ):\n",
    "        if current_token == None:\n",
    "            current_token = token_index\n",
    "            current_activations = np.zeros(4096)\n",
    "        if token_index != current_token:\n",
    "            if updated:\n",
    "                all_activations.append(current_activations)\n",
    "                all_token_indices.append(token_index - 1)  # FIXED OFF-BY-ONE ERROR\n",
    "                if len(all_activations) >= sample_limit:\n",
    "                    break\n",
    "            updated = False\n",
    "            current_token = token_index\n",
    "            current_activations = np.zeros(4096)\n",
    "        if sae_index in sae_neurons_in_cluster:\n",
    "            updated = True\n",
    "            current_activations += sae_value * decoder_vecs[sae_index]\n",
    "\n",
    "    return np.stack(all_activations), all_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef70a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d924d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_file = \"data/sae_activations_big_layer-7.npz\"\n",
    "layer = 7\n",
    "sample_limit = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4671c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Documents/EDA_LLM_Embeddings/code/Representation-Manifolds/RepresentationManifolds/lib/python3.11/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ae = get_gpt2_sae(layer=layer)\n",
    "decoder_vecs = ae.W_dec.data.cpu().numpy()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "sparse_activations = np.load(activations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729c1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_years = [1052, 2753, 4427, 6382, 8314, 9576, 9606, 13551, 19734, 20349]\n",
    "reconstructions_years, token_indices_years = get_cluster_activations_gpt2(sparse_activations, set(cluster_years), decoder_vecs)\n",
    "reconstructions_years, token_indices_years = reconstructions_years[:sample_limit], token_indices_years[:sample_limit]\n",
    "token_strs_years = tokenizer.batch_decode(sparse_activations['all_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3585655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_years = []\n",
    "for token_index_years in token_indices_years:\n",
    "    contexts_years.append(token_strs_years[max(0, token_index_years-10):token_index_years]) # thought it should be :token_index+1, but seems like there's an off-by-one error in Josh's script, so compensating here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191f1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subselect tokens corresponding to years\n",
    "\n",
    "years = []\n",
    "mask_years = []\n",
    "for context in contexts_years:\n",
    "    token = context[-1]\n",
    "    if token.strip().isdigit():\n",
    "        if 1900 <= int(token) <= 1999:\n",
    "            mask_years.append(True)\n",
    "            years.append(int(token.strip()))\n",
    "        else:\n",
    "            mask_years.append(False)\n",
    "    else:\n",
    "        mask_years.append(False)\n",
    "mask_years = np.array(mask_years)\n",
    "years = np.array(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1e2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_years = reconstructions_years[mask_years, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19747169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is in a 10-dimensional subspace of the 768-dimensional activation space so we reduce it to 10\n",
    "# dimensions using QR decomposition for space-efficiently.\n",
    "\n",
    "D_years = decoder_vecs[np.array(cluster_years), :]\n",
    "Q, _ = np.linalg.qr(D_years.T)\n",
    "X_years = X_years @ Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd39bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "X_years = pd.DataFrame(X_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363068fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_years.to_csv(\n",
    "    \"representations/years_reprs.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d51619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"label\": years\n",
    "}).to_csv(\n",
    "    \"representations/years_labels.csv\",\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742a615",
   "metadata": {},
   "source": [
    "Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0369f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_file = \"data/sae_activations_big_layer-8.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28a24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = get_mistral_sae(layer=8)\n",
    "decoder_vecs = sae.W_dec.detach().cpu().numpy()\n",
    "sparse_activations = np.load(activations_file)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "token_strs = tokenizer.convert_ids_to_tokens(sparse_activations[\"all_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "053d4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_days = [398, 35234, 31484, 54166, 52464, 23936, 20629]\n",
    "cluster_months = [7411, 33259, 49189, 46031, 9117, 57916, 26027, 16820, 41121, 23434, 39714, 59285,\n",
    "                  47182, 22809, 17555, 52568, 8934, 16406, 63163, 15477, 54144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6acd9dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 325405029/1000000000 [01:28<03:04, 3660752.00it/s]\n",
      "  5%|▌         | 53230130/1000000000 [00:11<03:19, 4753569.91it/s]\n"
     ]
    }
   ],
   "source": [
    "reconstructions_days, token_indices_days = get_cluster_activations_mistral(\n",
    "    sparse_activations, set(cluster_days), decoder_vecs, sample_limit=4_000, max_indices=1e9\n",
    ")\n",
    "reconstructions_months, token_indices_months = get_cluster_activations_mistral(\n",
    "    sparse_activations, set(cluster_months), decoder_vecs, sample_limit=4_000, max_indices=1e9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad201aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_week = {\n",
    "    \"monday\": 0,\n",
    "    \"mondays\": 0,\n",
    "    \"tuesday\": 1,\n",
    "    \"tuesdays\": 1,\n",
    "    \"wednesday\": 2,\n",
    "    \"wednesdays\": 2,\n",
    "    \"thursday\": 3,\n",
    "    \"thursdays\": 3,\n",
    "    \"friday\": 4,\n",
    "    \"fridays\": 4,\n",
    "    \"saturday\": 5,\n",
    "    \"saturdays\": 5,\n",
    "    \"sunday\": 6,\n",
    "    \"sundays\": 6,\n",
    "}\n",
    "\n",
    "months_of_year = {\n",
    "    \"january\": 0,\n",
    "    \"february\": 1,\n",
    "    \"march\": 2,\n",
    "    \"april\": 3,\n",
    "    \"may\": 4,\n",
    "    \"june\": 5,\n",
    "    \"july\": 6,\n",
    "    \"august\": 7,\n",
    "    \"september\": 8,\n",
    "    \"october\": 9,\n",
    "    \"november\": 10,\n",
    "    \"december\": 11,\n",
    "}\n",
    "\n",
    "days = []\n",
    "mask_days = []\n",
    "for i, token_i in enumerate(token_indices_days):\n",
    "    token = token_strs[token_i].replace(\"▁\", \"\").replace(\"▁\", \"\").lower().strip()\n",
    "    if token in days_of_week:\n",
    "        mask_days.append(True)\n",
    "        days.append(token)\n",
    "    else:\n",
    "        mask_days.append(False)\n",
    "days = days\n",
    "mask_days = np.array(mask_days)\n",
    "\n",
    "months = []\n",
    "mask_months = []\n",
    "for i, token_i in enumerate(token_indices_months):\n",
    "    token = token_strs[token_i].replace(\"▁\", \"\").replace(\"▁\", \"\").lower().strip()\n",
    "    if token in months_of_year:\n",
    "        mask_months.append(True)\n",
    "        months.append(token)\n",
    "    else:\n",
    "        mask_months.append(False)\n",
    "months = np.array(months)\n",
    "mask_months = np.array(mask_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09604a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_days = reconstructions_days[mask_days, :]\n",
    "\n",
    "D_days = decoder_vecs[np.array(cluster_days), :]\n",
    "Q_days, _ = np.linalg.qr(D_days.T)\n",
    "X_days = X_days @ Q_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "758dc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_days = pd.DataFrame(X_days)\n",
    "X_days.to_csv(\n",
    "    \"representations/days_reprs.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    ")\n",
    "# save labels\n",
    "pd.DataFrame({\n",
    "    \"label\": days\n",
    "}).to_csv(\n",
    "    \"representations/days_labels.csv\",\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9ceb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_months = reconstructions_months[mask_months, :]\n",
    "\n",
    "# remove outlier\n",
    "outlier_id = 696\n",
    "X_months = np.delete(X_months, outlier_id, axis=0)\n",
    "months = np.delete(months, outlier_id)\n",
    "\n",
    "D_months = decoder_vecs[np.array(cluster_months), :]\n",
    "Q_months, _ = np.linalg.qr(D_months.T)\n",
    "X_months = X_months @ Q_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beeb3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_months = pd.DataFrame(X_months)\n",
    "X_months.to_csv(\n",
    "    \"representations/months_reprs.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    ")\n",
    "# save labels\n",
    "pd.DataFrame({\n",
    "    \"label\": months\n",
    "}).to_csv(\n",
    "    \"representations/months_labels.csv\",\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f9356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RepresentationManifolds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
